{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0 is 42.504086215736095\n",
      "Cost at iteration 100 is 3.422107270032725\n",
      "Cost at iteration 200 is 0.2879499995206948\n",
      "Cost at iteration 300 is 0.036056420755789846\n",
      "Cost at iteration 400 is 0.015327671410188188\n",
      "Cost at iteration 500 is 0.013198197357009668\n",
      "Cost at iteration 600 is 0.012617198607926755\n",
      "Cost at iteration 700 is 0.012210587963068774\n",
      "Cost at iteration 800 is 0.01186204420652612\n",
      "Cost at iteration 900 is 0.011556842923360393\n",
      "Cost at iteration 1000 is 0.011289066396845668\n",
      "[[5.08920811]\n",
      " [2.83035319]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, print_cost=True):\n",
    "        self.learning_rate = 0.01\n",
    "        self.total_iterations = 1000\n",
    "        self.print_cost = print_cost\n",
    "\n",
    "    def y_hat(self, X, w):\n",
    "        # w是n行一列 转置后是一行n列，刚好和n行的X点积 算出 每一个特征值的预测值后求和\n",
    "        return np.dot(w.T, X)  \n",
    "\n",
    "    def cost(self, yhat, y):\n",
    "        # 损失函数采用均方误差\n",
    "        C = 1 / self.m * np.sum(np.power(yhat - y, 2))\n",
    "\n",
    "        return C\n",
    "\n",
    "    def gradient_descent(self, w, X, y, yhat):\n",
    "        # T表示转置 dot点积直接求和 dCdW表示对w求偏导\n",
    "        dCdW = 1 / self.m * np.dot(X, (yhat - y).T)  \n",
    "        w = w - self.learning_rate * dCdW\n",
    "\n",
    "        return w\n",
    "\n",
    "    def main(self, X, y):  # y = w1 + w2x2 + w3x3 + ...  ∵ x1 = 1所以w1 是 b\n",
    "        # Add x1 = 1\n",
    "        ones = np.ones((1, X.shape[1])) # x1在每一列的值都是1\n",
    "        X = np.append(ones, X, axis=0) # 行拼接\n",
    "\n",
    "        self.m = X.shape[1]  # 列 特征值数量\n",
    "        self.n = X.shape[0]  # 行 训练样本数量\n",
    "\n",
    "        w = np.zeros((self.n, 1))  # n行1列 w在每一行都是0\n",
    "\n",
    "        for it in range(self.total_iterations + 1):\n",
    "            yhat = self.y_hat(X, w) # 预测值\n",
    "            cost = self.cost(yhat, y) # 损失\n",
    "\n",
    "            if it % 100 == 0 and self.print_cost: \n",
    "                print(f\"Cost at iteration {it} is {cost}\") # 打印损失\n",
    "\n",
    "            w = self.gradient_descent(w, X, y, yhat)\n",
    "\n",
    "        return w\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.random.rand(1, 500) # 样本点\n",
    "    y = 3 * X + 5 + np.random.randn(1, 500) * 0.1  # 实际值\n",
    "    regression = LinearRegression() \n",
    "    w = regression.main(X, y) # 学习w\n",
    "    print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684b17d2421240f2bfb163267e97de2a4a7715e49eed0cbb8adb572a33a9a3cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
